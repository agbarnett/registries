---
title: "ANZCTR summary"
author: "Adrian Barnett"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  word_document: 
    toc: true
    toc_depth: 2
    reference_docx: rmarkdown-styles-reference.docx
---

```{r setup, include=FALSE}
# using formatting in Word document (see above)
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE, error=FALSE, comment='', dpi=400)
options(width=1000) # Wide pages
options(scipen=999) # avoid scientific presentation
library(dplyr)
library(broom)
library(stringr)
library(visdat) # for missing data
library(ggplot2)
library(gridExtra)
library(ggupset) # to plot combination of study characteristics
library(pander)
library(summarytools)
# global options for summary tools
st_options(plain.ascii = FALSE,       # Always use this option in Rmd documents
            style = "rmarkdown",        # This too
            round.digits = 0, 
            headings = FALSE, 
            footnote = NA,             # Avoids footnotes which would clutter the results
            subtitle.emphasis = FALSE  # Improves layout with some rmardown themes
) 

# graphics things:
g.theme = theme_bw() + theme(panel.grid.minor = element_blank())
cbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#999999", "#CC79A7")

# get the data
load('data/AnalysisReady.RData') # from 0_read_data.R 
source('99_functions.R')
```

These summary statistics are for trials registered in ANZCTR and not clinicaltrials.gov.

There were `r length(no.australian)` studies excluded because they were trials outside Australia or New Zealand.
The total number of remaining studies is `r format(nrow(studies),big.mark=',')`. 
The data were extracted from ANZCTR on `r censor.date`.

# Missing data

The columns are variables and the rows are observations.
The grey areas show missing data.

```{r, fig.width=8}
for.missing = dplyr::select(studies, -pub, -time, -ID, -starts_with('n_'))
vis_dat(for.missing)
```

# Registration and dates

## Registration dates

```{r, results='asis'}
dates = select(studies, submitted, approved, update) %>%
  tidyr::gather(key='date', value='res') %>%
  group_by(date) %>%
  filter(!is.na(res)) %>% # remove missing
  summarise(n=n(), 
            median = median(res), 
            q1 = as.Date(quantile(unclass(res), 0.25), origin = "1970-01-01"),
            q3 = as.Date(quantile(unclass(res), 0.75), origin = "1970-01-01")) %>%
  mutate(date = factor(date, levels=c('submitted','approved','update')),
         n = format(n,big.mark = ','),
         median = as.Date(median, origin='1970-01-01')) %>% # For row ordering
  arrange(date)
pander(dates, style='simple')
```

The summary statistics are the median and inter-quartile range.

## Anticipated start date

```{r}
stats = filter(studies, !is.na(start_anticipated)) %>%
  mutate(start_anticipated = as.numeric(start_anticipated)) %>%
        summarise(min=min(start_anticipated),
                  q1=quantile(start_anticipated, probs=0.25),
                  median=median(start_anticipated),
                  q3=quantile(start_anticipated, probs=0.75),
                  max=max(start_anticipated)) %>%
  mutate_all(funs(as.Date(., origin='1970-01-01')))
pander(stats, style='simple')
```

The summary statistics are the minimum, inter-quartile range, median and maximum.

# Inclusions/Exclusions

## Gender included

```{r, results='asis'}
with(studies, freq(gender, cumul = FALSE, report.nas =TRUE))
```

## Age included

#### Minimum age

```{r, results='asis'}
with(studies, freq(age_min_type, cumul = FALSE, report.nas =TRUE, order='freq'))
```

#### Minimum age limit in years

```{r}
# split by age
to.plot = filter(studies, !is.na(age_min)) 
# round to years for over 1
over_1 = filter(to.plot, age_min >=1) %>%
  mutate(age_min = round(age_min)) %>%
  group_by(age_min) %>%
  summarise(n = n()) %>%
  ungroup()
# round to months for under 1
under_1 = filter(to.plot, age_min < 1) %>%
  mutate(age_min = floor(age_min*12)) %>%
  group_by(age_min) %>%
  summarise(n = n()) %>%
  ungroup()
# separate plots
hplot1 = ggplot(under_1, aes(x=age_min, y=n))+
  geom_histogram(fill='darkseagreen', stat='identity')+
  scale_x_continuous(breaks=0:12)+
  xlab('Minimum age (months)')+
  ylab('Count')+
  g.theme+
  ggtitle('Age under 1')
hplot2 = ggplot(over_1, aes(x=age_min, y=n))+
  geom_histogram(fill='darkseagreen', stat='identity')+
  xlab('Minimum age (years)')+
  ylab('Count')+
  g.theme+
  ggtitle('Age 1 and over')
# combine plot
grid.arrange(hplot1, hplot2, ncol=2)
```

This plot is just for studies that had a minimum age limit.

#### Maximum age

```{r, results='asis'}
with(studies, freq(age_max_type, cumul = FALSE, report.nas =TRUE, order='freq'))
```

#### Maximum age limit in years

```{r}
# split by age
to.plot = filter(studies, !is.na(age_max)) 
# round to years for over 1
over_1 = filter(to.plot, age_max >=1) %>%
  mutate(age_max = round(age_max)) %>%
  group_by(age_max) %>%
  summarise(n = n()) %>%
  ungroup()
# round to months for under 1
under_1 = filter(to.plot, age_max < 1) %>%
  mutate(age_max = floor(age_max*12)) %>%
  group_by(age_max) %>%
  summarise(n = n()) %>%
  ungroup()
# separate plots
hplot1 = ggplot(under_1, aes(x=age_max, y=n))+
  geom_histogram(fill='dodgerblue', stat='identity')+
  scale_x_continuous(breaks=0:12)+
  xlab('Maximum age (months)')+
  ylab('Count')+
  g.theme+
  ggtitle('Age under 1')
hplot2 = ggplot(over_1, aes(x=age_max, y=n))+
  geom_histogram(fill='dodgerblue', stat='identity')+
  xlab('Maximum age (years)')+
  ylab('Count')+
  g.theme+
  ggtitle('Age 1 and over')
# combine plot
grid.arrange(hplot1, hplot2, ncol=2)
```

This plot is just for studies that had a maximum age limit.

# Study characteristics

## Study type

```{r, results='asis'}
with(studies, freq(studytype, cumul = FALSE, report.nas =TRUE, order='freq'))
```

## Purpose

```{r, results='asis'}
with(studies, freq(purpose, cumul = FALSE, report.nas =TRUE, order='freq'))
```

## Masking

```{r, results='asis'}
with(studies, freq(masking, cumul = FALSE, report.nas =TRUE, order='freq'))
```

## Intervention code

```{r, results='asis'}
with(studies, freq(intervention_code, cumul = FALSE, report.nas =TRUE, order='freq'))
```

## Control

```{r, results='asis'}
with(studies, freq(control, cumul = FALSE, report.nas =TRUE, order='freq'))
```

## Phase

```{r, results='asis'}
with(studies, freq(phase, cumul = FALSE, report.nas =TRUE))
```

## Endpoint

```{r, results='asis'}
with(studies, freq(endpoint, cumul = FALSE, report.nas =TRUE, order='freq'))
```

## Number of outcomes

```{r, results='asis'}
to.summary = select(studies, n_primary, n_secondary)
descr(to.summary, stats=c('fivenum'))
```

Number of primary and secondary outcomes.

# Health conditions

## Condition code #1

```{r, results='asis'}
with(studies, freq(ccode1, cumul = FALSE, report.nas =FALSE, order='freq')) 
```

## Condition code #2 (top 10)

```{r, results='asis'}
freq = group_by(studies, ccode2) %>%
  summarise(n = n()) %>%
  ungroup() %>%
  arrange(-n) %>%
  slice(1:10) # just top ten
to.table = mutate(studies,
                  ccode2 = ifelse(ccode2 %in% freq$ccode2, ccode2, 'Other')) # replace conditions outside top 10 with 'other'
with(to.table, freq(ccode2, cumul = FALSE, report.nas =FALSE, order='freq')) 
```

There are `r length(unique(studies$ccode2))` condition codes, hence we just show the top ten.
Conditions outside the top ten have been grouped together as 'other'.

## Condition as free text (top 10)

```{r, results='asis'}
freq = group_by(studies, condition) %>%
  summarise(n = n()) %>%
  ungroup() %>%
  arrange(-n) %>%
  slice(1:10) # just top ten
to.table = mutate(studies,
                  condition = ifelse(condition %in% freq$condition, condition, 'Other')) # replace conditions outside top 10 with 'other'
with(to.table, freq(condition, cumul = FALSE, report.nas =FALSE, order='freq')) 
```

There are `r length(unique(studies$condition))` conditions as free text, hence we just show the top ten.

This field is free text and so relies on what researchers write. Some categories could likely be combined, e.g., "prostate cancer" and "prostate cancer patients".

# Study status

## Study status

```{r, results='asis'}
with(studies, freq(study_status, cumul = FALSE, report.nas =FALSE, order='freq')) 
```

## Study-related publication listed

```{r, results='asis'}
with(studies, freq(pub, cumul = FALSE, report.nas =FALSE, order='freq')) 
```

This is just a check of whether there is a study publication listed. It does not check whether it is the main publication of the study results.

# Sponsors and funding

## Top institutes

```{r, results='asis'}
freq = group_by(studies, address) %>%
  summarise(n = n()) %>%
  ungroup() %>%
  arrange(-n) %>%
  slice(1:10) # just top ten
to.table = mutate(studies,
                  address = ifelse(address %in% freq$address, address, 'Other')) # replace those outside top 10 with 'other'
with(to.table, freq(address, cumul = FALSE, report.nas =FALSE, order='freq')) 
```

There are `r length(unique(studies$address))` institutes, hence we just show the top ten.

## Funding

### Number of funders

```{r, results='asis'}
with(studies, freq(n_funding, cumul = FALSE, report.nas =FALSE))
```

### Funding type

```{r, results='asis'}
with(funding_data, freq(type, cumul = FALSE, report.nas =FALSE, order='freq'))
```

This includes multiple results per study.

### Funders (top 10)

```{r, results='asis'}
freq = group_by(funding_data, name) %>%
  summarise(n = n()) %>%
  ungroup() %>%
  arrange(-n) %>%
  slice(1:10) # just top ten
to.table = mutate(funding_data,
                  name = ifelse(name %in% freq$name, name, 'Other')) # replace funders outside top 10 with 'other'
with(to.table, freq(name, cumul = FALSE, report.nas =FALSE, order='freq')) 
```

This includes multiple results per study.

There are `r length(unique(funding_data$name))` funders, hence we just show the top ten.


# Sample size

## Target and achieved sample size

```{r, results='asis'}
sample = select(studies, samplesize_target, samplesize_actual) %>%
  rename('Target' = 'samplesize_target',
         'Achieved' = 'samplesize_actual')
descr(sample, stats='fivenum')
zero.studies = nrow(filter(studies, samplesize_target == 0))
```

The table shows summary statistics.

There were `r zero.studies` studies with a zero target sample size.


## Histogram of sample size

```{r}
exclude_large = filter(studies, samplesize_target < 2000) # exclude extremely large genetics study
hplot = ggplot(exclude_large, aes(x=samplesize_target))+
  geom_histogram(fill='chocolate1')+
  xlab('Target sample size')+
  g.theme
hplot
```

The plot above is for the `r format(nrow(exclude_large), big.mark=',')` studies with a target sample size under 2,000.


### Histogram looking for digit preference in target sample size

```{r, fig.width=8}
#
exclude_large = filter(studies, samplesize_target <= 200) # exclude extremely large genetics study
# 
digits = group_by(exclude_large, samplesize_target) %>%
  summarise(n=n()) %>%
  ungroup() %>%
  arrange(-n) %>%
  slice(1:10) # top 10
# plot
hplot = ggplot(exclude_large, aes(x=samplesize_target))+
  geom_histogram(fill='chocolate2', breaks=0:200)+
  xlab('Target sample size')+
  geom_label(data=digits, aes(x=samplesize_target, y=n, label=samplesize_target))+
  g.theme
hplot
jpeg('figures/digit_preference.jpg', width=5, height=4, units='in', res=300)
print(hplot)
invisible(dev.off())
```

The plot above is for the `r format(nrow(exclude_large), big.mark=',')` studies with a target sample size of 200 or fewer.

There is a strong digit preference at multiples of ten. The modal sample size is `r digits$samplesize_target[1]`.

### Comparing target and achieved sample size

Below we show a Bland-Altman plot using all the data on the original scale. The results are dominated by a few very large studies, hence we need to re-scale the data.

```{r}
to_plot = filter(studies, !is.na(samplesize_actual)) %>%
  mutate(diff = samplesize_actual - samplesize_target,
         aver = (samplesize_actual + samplesize_target)/2 )
ba.plot = ggplot(data=to_plot, aes(x=aver, y=diff))+
  geom_point(pch=1)+
  xlab('Average sample size')+
  ylab('Difference (Actual minus Target)')+
  g.theme
ba.plot 
```

### Restricted plot to exclude outliers

```{r}
# stats
z = qnorm(0.975)
stats = filter(to_plot, aver < 1000) %>%
  summarise(mean=mean(diff), sd=sd(diff)) %>%
  mutate(lower = mean - z*sd, # limits of agreement
         upper = mean + z*sd)
# plot
ba.plot + coord_cartesian(xlim=c(0,1000), ylim=c(-2000,2000))+
  geom_hline(lty=2, col='red', yintercept = stats$mean)+
  geom_hline(lty=2, col='green', yintercept = stats$lower)+
  geom_hline(lty=2, col='green', yintercept = stats$upper)
```

The Bland-Altman plot above is restricted to exclude the few outliers from very large studies.

### Comparing target and achieved sample size

We use a log-transformation (base e) to remove the strong skew in the sample sizes.

```{r}
# log-transformed version
to_plot = filter(studies, 
                 !is.na(samplesize_actual)) %>% # remove missing
  mutate(
    samplesize_target_log = log(samplesize_target+0.1), # add small constant because of zeros
    samplesize_actual_log = log(samplesize_actual+0.1),
    diff = samplesize_actual_log - samplesize_target_log,
    diff_perc = 100*(exp(diff)-1), # percent difference
    aver = (samplesize_actual_log + samplesize_target_log)/2 )
# check
#filter(to_plot, diff< log(0.1)) %>% select(samplesize_target, samplesize_actual) # where sample was 10% of target
# stats
stats = summarise(to_plot, mean=mean(diff), sd=sd(diff)) %>%
  mutate(lower = mean - z*sd, # limits of agreement
         upper = mean + z*sd)
# label log axis
x.labels = c(1,5,50,500,5000,50000)
x.ticks = log(x.labels+1)
y.labels = c(0.1,0.25,1,4,10) # relative (like a risk ratio); symmetric
y.ticks = log(y.labels)
# plot
ba.plot_log2 = ggplot(data=to_plot, aes(x=aver, y=diff))+
  geom_point(pch=1)+
  scale_x_continuous(breaks=x.ticks, labels=x.labels)+
  scale_y_continuous(breaks=y.ticks, labels=y.labels)+
  xlab('Average sample size (log scale)')+
  ylab('Relative difference (Actual minus Target)')+
  geom_hline(lty=2, col='red', yintercept = stats$mean)+
  geom_hline(lty=2, col='green', yintercept = stats$lower)+
  geom_hline(lty=2, col='green', yintercept = stats$upper)+
  g.theme
ba.plot_log2 
```

The Bland-Altman plot above shows the 95% limits of agreement as horizontal green lines, which are `r roundz(exp(stats$lower,2))` to `r roundz(exp(stats$upper,2))`. 

### Bland-Altman plot for sample size difference using a density plot

The dots in the above plot make it hard to see where most of the observations are.
The plot below shows the density.

```{r}
to_tile = mutate(to_plot,
                 diff_bin = round(diff),
                 diff_aver = round(aver*3)/3) %>%
  group_by(diff_bin, diff_aver) %>%
  summarise(n=n()) %>%
  ungroup()
tile_plot = ggplot(data=to_tile, aes(x=diff_aver, y=diff_bin, fill=n))+
  geom_tile()+
  scale_x_continuous(breaks=x.ticks, labels=x.labels)+
  scale_y_continuous(breaks=y.ticks, labels=y.labels)+
  scale_fill_viridis_c()+
  xlab('Average (log scale)')+
  ylab('Relative difference (Actual minus Target)')+
  g.theme
tile_plot
```

## Regression model of difference against average

We used a Bayesian model to examine the difference in the sample size (actual minus target) against the average sample size.

```{r}
# get the bayesian results
load('results/bland_altman_bayes_model.RData') # from 2_bayes_model_sample_diff.R
res = data.frame(bugs.results$summary[,c(1,3,7)])
names(res) = c('mean','lower','upper'); res$var = row.names(res)
# regression model for tau
reg_parms = filter(res, str_detect(string=var, pattern='alpha|beta'))
reg_parms_nice = mutate(reg_parms, cell = paste(roundz(mean,2), ' (', roundz(lower,2), ' to ', roundz(upper,2), ')', sep=''),
         Term = case_when(
           str_detect(var, 'alpha') ~ 'Mean',
           str_detect(var, 'beta') ~ 'Precision'
         ),
         var = case_when(
           str_detect(var, '1') ~ 'Intercept',
           str_detect(var, '2') ~ 'Slope',
           str_detect(var, '3') ~ 'Quadratic'
         ))  %>%
  select(Term, var, cell) %>%
  rename('Variable' = 'var',
         'Mean (95% CI)' = 'cell')
pander(reg_parms_nice, style='simple')
# mean difference and CI (for text below) 
mean.diff = filter(res, var=='alpha')
```

The results above are the estimated parameters from the regression model fitted to the data shown in the Bland-Altman plot.
The regression was the difference against the average, and we allowed both the mean and the precision (inverse-variance) to depend on the average sample size.
The means and 95% CIs from the above table show that both the intercept and quadratic were statistically significant for the mean and precision. So there was a non-linear change in mean and the precision.

The regression model results above are relatively difficult to interpret, so we plot the estimates below. The mean difference is the solid line and the shaded area is a 95% credible interval for the difference which highlights the changing precision. The plot also shows the studies that fall outside the 95% credible interval limits.

```{r}
# un-centre predication locations
preds_av = av.pred + mean_aver
# extract bayes parameters
alpha_1 = filter(reg_parms, var=='alpha[1]')$mean
alpha_2 = filter(reg_parms, var=='alpha[2]')$mean
alpha_3 = filter(reg_parms, var=='alpha[3]')$mean
beta_1 = filter(reg_parms, var=='beta[1]')$mean
beta_2 = filter(reg_parms, var=='beta[2]')$mean
beta_3 = filter(reg_parms, var=='beta[3]')$mean
# add outliers by reconstructing bayes limits
add_outliers = select(to_plot, aver, diff) %>%
  mutate(
  c_aver = aver - mean_aver,
  est_mean = alpha_1 + (c_aver * alpha_2) + (c_aver * c_aver *alpha_3),
  est_tau = beta_1 + (c_aver * beta_2) + (c_aver * c_aver *beta_3),
  est_sigma = 1/sqrt(est_tau),
  z = qnorm(0.90),
  lower = est_mean - (z*est_sigma),
  upper = est_mean + (z*est_sigma)
) %>%
filter(diff > upper | diff < lower)
# to here, add outlier studies; dotted line at 1
to_plot_diff = filter(res, str_detect(string=var, pattern='diff')) %>%
  mutate(xaxis= preds_av)
plot = ggplot(data=to_plot_diff, aes(x=xaxis, y=mean, ymin=lower, ymax=upper))+
  geom_ribbon(alpha=0.2)+
  geom_line(size=1.05)+
  geom_point(data=add_outliers, aes(x=aver, y=diff))+
  scale_x_continuous(breaks=x.ticks, labels=x.labels)+
  scale_y_continuous(breaks=y.ticks, labels=y.labels)+
  xlab('Average (log scale)')+
  ylab('Relative difference (Actual minus Target)')+
  g.theme
plot
```

The averaged difference on the scale of relative was estimated as `r roundz(exp(mean.diff$mean),2)` with a 95% credible interval of `r roundz(exp(mean.diff$lower),2)` to `r roundz(exp(mean.diff$upper),2)`. So on average, sample sizes are `r roundz(100*(exp(mean.diff$mean)-1),2)`% smaller than their target. However, the mean difference is larger for smaller studies, and for larger studies the mean difference is close to 1.

The variance in the difference is narrowest for sample sizes around 500, suggesting studies of this size more often have an actual sample size that is nearer the target.

There are more individual studies that fall outside the credible intervals that are below the shaded area. These are studies that recruited far fewer patients than expected. A lot of these points are between 5 and 500. 

## Multiple regression model of sample size

```{r, include=FALSE}
# check if results are already available
is_results = length(dir('results', pattern = 'ANZCTR_sample_size')) > 0
if(is_results == TRUE){
  load('results/ANZCTR_sample_size.RData')
}
if(is_results == FALSE){
  source('2_elasticnet_model_samplesize.R')
}
```

We used elastic net. Add more.
Checked variables using the VIF and knocked out two variables that were strongly colinear with observational study design.


### Plot of relative differences

```{r, fig.width=8, fig.height=9}
# make table of estimates and names, and add reference groups
# some results put in order of high to low
table_names = read.table(sep='!', header=TRUE, stringsAsFactors = FALSE, text='
xaxis!group!term!label!reference
1!continuous!date!Trend per 5 years!FALSE
2!continuous!n_primary!Double the number of primary outcomes!FALSE
3!continuous!n_secondary!Double the number of secondary outcomes!FALSE
4!continuous!n_funding!Number of funders!FALSE
5!Gender!genderMales!Males!FALSE
6!Gender!genderFemales!Females!FALSE
7!Gender!genderBoth males and females!All!TRUE
8!Age limit!age_limitOver 18!Over 18!FALSE
9!Age limit!age_limitExactly 18!Exactly 18!FALSE
10!Age limit!age_limitUnder 18!Under 18!FALSE
11!Age limit!age_limitNo limit!No limit!TRUE
12!Type!studytypeInterventional!Interventional!TRUE
13!Type!studytypeObservational!Observational!FALSE
14!Phase!phasePhase 0!Phase 0!FALSE
15!Phase!phasePhase 1!Phase 1!FALSE
16!Phase!phasePhase 1 / Phase 2!Phase 1/2!FALSE
17!Phase!phasePhase 2!Phase 2!FALSE
18!Phase!phasePhase 2 / Phase 3!Phase 2/3!FALSE
19!Phase!phasePhase 3!Phase 3!TRUE
20!Phase!phasePhase 3 / Phase 4!Phase 3/4!FALSE
21!Phase!phaseMissing!Missing!FALSE
22!Endpoint!endpointBio-availability!Bio-availability!FALSE
23!Endpoint!endpointPharmacodynamics!Pharmacodynamics!FALSE
24!Endpoint!endpointPharmacokinetics!Pharmacokinetics!FALSE
25!Endpoint!endpointPharmacokinetics / pharmacodynamics!Pharmacokinetics / pharmacodynamics!FALSE
26!Endpoint!endpointBio-equivalence!Bio-equivalence!FALSE
27!Endpoint!endpointSafety!Safety!FALSE
28!Endpoint!endpointMissing!Missing!FALSE
29!Endpoint!endpointEfficacy!Efficacy!TRUE
30!Purpose!purposeTreatment!Treatment!TRUE
31!Purpose!purposeDiagnosis!Diagnosis!FALSE
32!Purpose!purposeEducational / counselling / training!Educational / counselling / training!FALSE
33!Purpose!purposePrevention!Prevention!FALSE
34!Masking!maskingOpen (masking not used)!Open!FALSE
35!Masking!maskingBlinded (masking used)!Blinded!TRUE
36!Control!controlMissing!Missing!FALSE
37!Control!controlUncontrolled!Uncontrolled!FALSE
38!Control!controlDose comparison!Dose comparison!FALSE
39!Control!controlPlacebo!Placebo!FALSE
40!Control!controlActive!Active!TRUE
41!Control!controlHistorical!Historical!FALSE
42!Intervention!intervention_codeMissing!Missing!FALSE
43!Intervention!intervention_codeRehabilitation!Rehabilitation!FALSE
44!Intervention!intervention_codeTreatment: Devices!Treatment: Devices!FALSE
45!Intervention!intervention_codeLifestyle!Lifestyle!FALSE
46!Intervention!intervention_codeTreatment: Other!Treatment: Other!FALSE
47!Intervention!intervention_codeNone!None!FALSE
48!Intervention!intervention_codeTreatment: Drugs!Treatment: Drugs!TRUE
49!Intervention!intervention_codeDiagnosis / Prognosis!Diagnosis / Prognosis!FALSE
50!Intervention!intervention_codePrevention!Prevention!FALSE
51!Intervention!intervention_codeEarly detection / Screening!Early detection / Screening!FALSE
52!Area!ccode1Human Genetics and Inherited Disorders!Human Genetics and Inherited Disorders!FALSE
53!Area!ccode1Metabolic and Endocrine!Metabolic and Endocrine!FALSE
54!Area!ccode1Eye!Eye!FALSE
55!Area!ccode1Neurological!Neurological!FALSE
56!Area!ccode1Physical Medicine / Rehabilitation!Physical Medicine / Rehabilitation!FALSE
57!Area!ccode1Alternative and Complementary Medicine!Alternative and Complementary Medicine!FALSE
58!Area!ccode1Diet and Nutrition!Diet and Nutrition!FALSE
59!Area!ccode1Respiratory!Respiratory!FALSE
60!Area!ccode1Oral and Gastrointestinal!Oral and Gastrointestinal!FALSE
61!Area!ccode1Renal and Urogenital!Renal and Urogenital!FALSE
62!Area!ccode1Musculoskeletal!Musculoskeletal!FALSE
63!Area!ccode1Skin!Skin!FALSE
64!Area!ccode1Cardiovascular!Cardiovascular!TRUE
65!Area!ccode1Mental Health!Mental Health!FALSE
66!Area!ccode1Cancer!Cancer!FALSE
67!Area!ccode1Injuries and Accidents!Injuries and Accidents!FALSE
68!Area!ccode1Infection!Infection!FALSE
69!Area!ccode1Reproductive Health and Childbirth!Reproductive Health and Childbirth!FALSE
70!Area!ccode1Public Health!Public Health!FALSE
71!Area!ccode1Emergency medicine!Emergency medicine!FALSE
') %>%
  arrange(xaxis) # sort in order for labels
# make reference groups
reference_groups = filter(table_names, reference==TRUE)  %>%
 mutate(estimate=0)

# add above to estimates
add_ests = filter(table_names, reference==FALSE) %>%
  left_join(standard_model_ests, by='term') %>%
  bind_rows(reference_groups) %>% # add references
  mutate(
    # back transform to relative change:
    estimate = exp(estimate), 
    conf.low = exp(conf.low),
    conf.high = exp(conf.high)
  ) %>%
  select(xaxis, estimate, conf.low, conf.high, reference) %>%
  mutate(reference = as.numeric(reference) + 1) # to mark reference point
axis_labels = table_names$label

# quick check that all estimates are in the reference table (should only be intercept); and vice versa
check_merge = anti_join(standard_model_ests, table_names, by='term')
check_merge_reverse = anti_join(table_names, standard_model_ests, by='term') %>%
  filter(reference == FALSE)

# labels for groups
group_labels = group_by(table_names, group) %>%
  summarise(n=n(), meanx=mean(xaxis), maxx=max(xaxis)) %>%
  ungroup() %>%
  filter(n > 1) %>%
  mutate(
    estimate = max(add_ests$conf.high, na.rm=T), conf.low=0, conf.high=0, reference=1,
    group = ifelse(group=='continuous', 'Continuous\noutcomes', group)) 
dotted.lines = group_labels$maxx + 0.5 # dotted lines to split groups
# text for axis labels
text1 = data.frame(xaxis=0, estimate=1, conf.low=0, conf.high=0, reference=1, label='Increase')
text2 = data.frame(xaxis=0, estimate=1, conf.low=0, conf.high=0, reference=1, label='Decrease')
# plot
star.wars.relative = ggplot(data=add_ests, aes(x=xaxis, y=estimate, ymin=conf.low, ymax=conf.high, shape=factor(reference)))+
  geom_point(size=2)+
  scale_shape_manual(NULL, values=c(19,17))+ # triangle and circle
  geom_errorbar(width=0, size=1.02)+
  geom_hline(lty=2, yintercept=1)+
  geom_vline(lty=3, xintercept=c(4.5, dotted.lines))+ # breaks between categorical variables
  geom_text(data=text1, aes(x=xaxis, y=estimate, label =label), adj=-0.1, col=grey(0.5))+
  geom_text(data=text2, aes(x=xaxis, y=estimate, label =label), adj=1.1, col=grey(0.5))+
  geom_text(data=group_labels, aes(x=meanx, y=estimate, label =group), adj=1, col=grey(0.5))+
  scale_x_continuous(expand=c(0.01,0.01), breaks=1:length(axis_labels), labels=axis_labels, limits=c(0, length(axis_labels)))+
  scale_y_continuous(breaks=seq(0,3,1), minor_breaks = seq(0,3,0.5), limits=c(0.25, NA))+ # avoid showing 0 on RR axis
  ylab('Relative change in sample size')+
  xlab('')+
  theme_bw()+
  theme(legend.position = 'none',
        text=element_text(size=13), 
        panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank())+
  coord_flip() # to 
star.wars.relative
# export to tall-thin plot
jpeg('figures/ANZCTR_sample_size_relative.jpg', width=6, height=9.5, units='in', res=300)
print(star.wars.relative)
invisible(dev.off())
```


### Plot of absolute differences

```{r, fig.width=8, fig.height=6}
#To do
```

We log-transformed (base e) the sample size because of a very strong positive skew.
The plot shows the estimated mean and 95% confidence interval on the untransformed scale.

### Check residuals from regression model

```{r, fig.width=3.5, fig.height=3.5}
res = data.frame(res = resid(standard_model))
rplot = ggplot(res, aes(x=res))+
  geom_histogram(fill='firebrick3')+
  g.theme+
  xlab('Residual (log-scale)')
rplot
# r-squared
# rsq = cor(log2(for.model$samplesize_target), fitted(smodel)) ^2
```

## Common characteristics

Below we show the top ten characteristics of studies in terms of study type, purpose and masking.

```{r, fig.width=8, fig.height=6}
# get combinations 
combs <-  dplyr::select(studies, purpose, masking, studytype, ID)  %>%
  as_tibble() %>%
  tidyr::gather(key='characteristic', value='response', -ID) %>%
  filter(!is.na(response),
         response != 'Not Applicable') %>%
  dplyr::select(-characteristic)
# now make list from group responses
list_resp = group_by(combs, ID) %>%
  summarise(responses = list(response))
# plot
cplot = ggplot(list_resp, aes(x = responses)) +
    geom_bar(aes(y=..count../sum(..count..)), fill = "indianred3") +
    theme_bw() +
    xlab("Study characteristics") +
    ylab("Proportion of studies") +
    scale_x_upset(n_intersections = 10)+
    g.theme+
  theme(text=element_text(size=14),
        plot.margin = margin(t = 0, r = 0, b = 0, l = 120, unit = "pt")) # extend margin for labels
cplot
```


## Principal components analysis

```{r}
library(PCAmixdata)
# to do: would be better to turn age_min and age_max into numbers
# use small subset of variables
for.pca = select(studies, submitted, studytype, assignment, endpoint, control, n_primary, n_secondary ) 
# split by continuous and categorical
split <- splitmix(for.pca) # split into quant and qual data
X1 <- split$X.quanti 
X2 <- split$X.quali 
res.pcamix <- PCAmix(X.quanti=X1, X.quali=X2, rename.level=TRUE, graph=FALSE)
```

```{r, fig.width=8, fig.height=8}
# plot the PCA results
par(mfrow=c(2,2))
plot(res.pcamix,choice="ind",coloring.ind=X2$houses,label=FALSE,
      posleg="bottomright", main="Observations")
plot(res.pcamix, choice="levels", xlim=c(-1.5,2.5), main="Levels")
plot(res.pcamix, choice="cor", main="Numerical variables")
plot(res.pcamix, choice="sqload", coloring.var=T, leg=TRUE,
     posleg="topright", main="All variables")
```

# Appendix

## Convergence of Bayesian estimates

```{r}
```
