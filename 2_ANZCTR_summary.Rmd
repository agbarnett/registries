---
title: "ANZCTR summary"
author: "Adrian Barnett"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  word_document: 
    toc: true
    toc_depth: 2
    reference_docx: rmarkdown-styles-reference.docx
---

```{r setup, include=FALSE}
# using formatting in Word document (see above)
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, message=FALSE, error=FALSE, comment='', dpi=400)
options(width=1000) # Wide pages
options(scipen=999) # avoid scientific presentation
library(dplyr)
library(broom)
library(stringr)
library(visdat) # for missing data
library(ggplot2)
library(gridExtra)
library(ggupset) # to plot combination of study characteristics
library(flextable)
library(summarytools)
# global options for summary tools
st_options(plain.ascii = FALSE,       # Always use this option in Rmd documents
            style = "rmarkdown",        # This too
            round.digits = 0, 
            headings = FALSE, 
            footnote = NA,             # Avoids footnotes which would clutter the results
            subtitle.emphasis = FALSE  # Improves layout with some rmardown themes
) 

# graphics things:
g.theme = theme_bw() + theme(panel.grid.minor = element_blank())
cbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#999999", "#CC79A7")

# get the data
load('data/AnalysisReady.RData') # from 0_read_data_anzctr.R 
source('99_functions.R')

# remove provisional?

```

These summary statistics are for trials registered in ANZCTR and not clinicaltrials.gov.

The total number of studies is `r format(nrow(studies),big.mark=',')`. 
The data were extracted from ANZCTR on `r censor.date`.

# Missing data

The columns are variables and the rows are observations.
The grey areas show missing data.

```{r, fig.width=8}
for.missing = dplyr::select(studies, -pub, -time, -ID, -id, -starts_with('n_'))
vis_dat(for.missing)
```

# Registration and dates

## Registration dates

```{r, results='asis'}
dates = select(studies, submitted, approved, update) %>%
  tidyr::gather(key='date', value='res') %>%
  group_by(date) %>%
  filter(!is.na(res)) %>% # remove missing
  summarise(n=n(), 
            median = median(res), 
            q1 = as.Date(quantile(unclass(res), 0.25), origin = "1970-01-01"),
            q3 = as.Date(quantile(unclass(res), 0.75), origin = "1970-01-01")) %>%
  mutate(date = factor(date, levels=c('submitted','approved','update')),
         n = format(n,big.mark = ','),
         median = as.Date(median, origin='1970-01-01'),
         median = format(median, '%b %Y'), # just month and year
         q1 = format(q1, '%b %Y'),
         q3 = format(q3, '%b %Y')) %>% 
  arrange(date)
ftab = flextable(dates) %>%
  theme_box() %>%
  autofit()
ftab
```

The summary statistics are the median and inter-quartile range.

## Anticipated start date

```{r}
stats = filter(studies, !is.na(start_anticipated)) %>%
  mutate(start_anticipated = as.numeric(start_anticipated)) %>%
        summarise(min=min(start_anticipated),
                  q1=quantile(start_anticipated, probs=0.25),
                  median=median(start_anticipated),
                  q3=quantile(start_anticipated, probs=0.75),
                  max=max(start_anticipated)) %>%
  mutate_all(funs(as.Date(., origin='1970-01-01'))) %>%
  mutate_all(funs(format(., '%b %Y'))) # change all to month - year
ftab = flextable(stats) %>%
  theme_box() %>%
  autofit()
ftab
```

The summary statistics are the minimum, inter-quartile range, median and maximum.

# Inclusions/Exclusions

## Gender included

```{r, results='asis'}
with(studies, freq(gender, cumul = FALSE, report.nas =TRUE))
```

## Age included

#### Minimum age

```{r, results='asis'}
with(studies, freq(age_min_type, cumul = FALSE, report.nas =TRUE, order='freq'))
```

#### Minimum age limit in years

```{r, fig.width=7}
# split by age
to.plot = filter(studies, !is.na(age_min)) 
# round to years for over 1
over_1 = filter(to.plot, age_min >=1) %>%
  mutate(age_min = round(age_min)) %>%
  group_by(age_min) %>%
  summarise(n = n()) %>%
  ungroup()
# round to months for under 1
under_1 = filter(to.plot, age_min < 1) %>%
  mutate(age_min = floor(age_min*12)) %>%
  group_by(age_min) %>%
  summarise(n = n()) %>%
  ungroup()
# separate plots
hplot1 = ggplot(under_1, aes(x=age_min, y=n))+
  geom_histogram(fill='darkseagreen', stat='identity')+
  scale_x_continuous(breaks=0:12)+
  xlab('Minimum age (months)')+
  ylab('Count')+
  g.theme+
  ggtitle('Age under 1')
hplot2 = ggplot(over_1, aes(x=age_min, y=n))+
  geom_histogram(fill='darkseagreen', stat='identity')+
  xlab('Minimum age (years)')+
  ylab('Count')+
  g.theme+
  ggtitle('Age 1 and over')
# combine plot
grid.arrange(hplot1, hplot2, ncol=2)
```

This plot is just for the `r format(nrow(to.plot), big.mark=',')` studies that had a minimum age limit.
By far the most common minimum age is 18.
The oldest exclusion for minimum age was `r max(to.plot$age_min)` years.

#### Maximum age

```{r, results='asis'}
with(studies, freq(age_max_type, cumul = FALSE, report.nas =TRUE, order='freq'))
```

#### Maximum age limit in years

```{r, fig.width=7}
# split by age
to.plot = filter(studies, !is.na(age_max)) 
# round to years for over 1
over_1 = filter(to.plot, age_max >=1) %>%
  mutate(age_max = round(age_max)) %>%
  group_by(age_max) %>%
  summarise(n = n()) %>%
  ungroup()
# round to months for under 1
under_1 = filter(to.plot, age_max < 1) %>%
  mutate(age_max = floor(age_max*12)) %>%
  group_by(age_max) %>%
  summarise(n = n()) %>%
  ungroup()
# separate plots
hplot1 = ggplot(under_1, aes(x=age_max, y=n))+
  geom_histogram(fill='dodgerblue', stat='identity')+
  scale_x_continuous(breaks=0:12)+
  xlab('Maximum age (months)')+
  ylab('Count')+
  g.theme+
  ggtitle('Age under 1')
hplot2 = ggplot(over_1, aes(x=age_max, y=n))+
  geom_histogram(fill='dodgerblue', stat='identity')+
  xlab('Maximum age (years)')+
  ylab('Count')+
  g.theme+
  ggtitle('Age 1 and over')
# combine plot
grid.arrange(hplot1, hplot2, ncol=2)
```


This plot is just for the `r format(nrow(to.plot), big.mark=',')` studies that had a maximum age limit.
The oldest exclusion for maximum age was `r max(to.plot$age_max)` years.

There is a strong digit preference in the age limit with spikes in multiples of five years.

# Study characteristics

## Study type

```{r, results='asis'}
with(studies, freq(study_type, cumul = FALSE, report.nas =TRUE, order='freq'))
```

#### Study type over time

```{r}
to_plot = mutate(studies,
                 year = as.numeric(format(submitted, '%Y'))) %>% # make year
  group_by(year, study_type) %>%
  tally()
gplot = ggplot(data=to_plot, aes(x=year, y=n, fill=study_type))+
  geom_bar(stat='identity')+
  scale_fill_manual(NULL, values=c('darkseagreen3','darkgoldenrod2'))+
  theme_bw()+
  ylab('Number of studies')+
  xlab('Year submitted')+
  theme(legend.position = c(0.15,0.8))
gplot
```

## Purpose

```{r, results='asis'}
with(studies, freq(purpose, cumul = FALSE, report.nas =TRUE, order='freq'))
```

## Masking

```{r, results='asis'}
with(studies, freq(masking, cumul = FALSE, report.nas =TRUE, order='freq'))
```

## Intervention code

```{r, results='asis'}
with(studies, freq(intervention_code, cumul = FALSE, report.nas =TRUE, order='freq'))
```

## Control

```{r, results='asis'}
with(studies, freq(control, cumul = FALSE, report.nas =TRUE, order='freq'))
```

## Phase

```{r, results='asis'}
with(studies, freq(phase, cumul = FALSE, report.nas =TRUE))
```

## Endpoint

```{r, results='asis'}
with(studies, freq(endpoint, cumul = FALSE, report.nas =TRUE, order='freq'))
```

## Number of outcomes

```{r, results='asis'}
to.summary = select(studies, n_primary, n_secondary)
descr(to.summary, stats=c('fivenum'))
```

Number of primary and secondary outcomes.

# Health conditions

## Condition code #1

```{r, results='asis'}
with(studies, freq(ccode1, cumul = FALSE, report.nas =FALSE, order='freq')) 
```

## Condition code #2 (top 10)

```{r, results='asis'}
freq = group_by(studies, ccode2) %>%
  summarise(n = n()) %>%
  ungroup() %>%
  arrange(-n) %>%
  slice(1:10) # just top ten
to.table = mutate(studies,
                  ccode2 = ifelse(ccode2 %in% freq$ccode2, ccode2, 'Other')) # replace conditions outside top 10 with 'other'
with(to.table, freq(ccode2, cumul = FALSE, report.nas =FALSE, order='freq')) 
```

There are `r length(unique(studies$ccode2))` condition codes, hence we just show the top ten.
Conditions outside the top ten have been grouped together as 'other'.

## Condition as free text (top 10)

```{r, results='asis'}
freq = group_by(studies, condition) %>%
  summarise(n = n()) %>%
  ungroup() %>%
  arrange(-n) %>%
  slice(1:10) # just top ten
to.table = mutate(studies,
                  condition = ifelse(condition %in% freq$condition, condition, 'Other')) # replace conditions outside top 10 with 'other'
with(to.table, freq(condition, cumul = FALSE, report.nas =FALSE, order='freq')) 
```

There are `r format(length(unique(studies$condition)), big.mark=',')` conditions as free text, hence we just show the top ten.

This field is free text and so relies on what researchers write. Some categories could likely be combined, e.g., "prostate cancer" and "prostate cancer patients".

# Study status

## Study status

```{r, results='asis'}
with(studies, freq(study_status, cumul = FALSE, report.nas =FALSE, order='freq')) 
```

## Provisional

```{r, results='asis'}
with(studies, freq(provisional, cumul = FALSE, report.nas =FALSE, order='freq')) 
```

## Study-related publication listed

```{r, results='asis'}
with(studies, freq(pub, cumul = FALSE, report.nas =FALSE, order='freq')) 
```

This is just a check of whether there is a study publication listed. It does not check whether it is the main publication of the study results.

# Sponsors and funding

## Top institutes

```{r, results='asis'}
freq = group_by(studies, address) %>%
  summarise(n = n()) %>%
  ungroup() %>%
  arrange(-n) %>%
  slice(1:10) # just top ten
to.table = mutate(studies,
                  address = ifelse(address %in% freq$address, address, 'Other')) # replace those outside top 10 with 'other'
with(to.table, freq(address, cumul = FALSE, report.nas =FALSE, order='freq')) 
```

There are `r format(length(unique(studies$address)), big.mark=',')` institutes, hence we just show the top ten.

## Funding

### Number of funders

```{r, results='asis'}
with(studies, freq(n_funding, cumul = FALSE, report.nas =FALSE))
```

### Funding type

```{r, results='asis'}
with(funding_data, freq(type, cumul = FALSE, report.nas =FALSE, order='freq'))
```

This includes multiple results per study.

### Funders (top 10)

```{r, results='asis'}
freq = group_by(funding_data, name) %>%
  summarise(n = n()) %>%
  ungroup() %>%
  arrange(-n) %>%
  slice(1:10) # just top ten
to.table = mutate(funding_data,
                  name = ifelse(name %in% freq$name, name, 'Other')) # replace funders outside top 10 with 'other'
with(to.table, freq(name, cumul = FALSE, report.nas =FALSE, order='freq')) 
```

This includes multiple results per study.

There are `r format(length(unique(funding_data$name)), big.mark=',')` funders, hence we just show the top ten.


# Sample size

## Target and achieved sample size

```{r, results='asis'}
sample = select(studies, samplesize_target, samplesize_actual) %>%
  rename('Target' = 'samplesize_target',
         'Achieved' = 'samplesize_actual')
descr(sample, stats='fivenum')
zero.studies = nrow(filter(studies, samplesize_target == 0))
```

The table shows summary statistics.

There were `r zero.studies` studies with a zero target sample size.


## Histogram of sample size

```{r}
exclude_large = filter(studies, samplesize_target < 2000) # exclude extremely large genetics study
hplot = ggplot(exclude_large, aes(x=samplesize_target))+
  geom_histogram(fill='chocolate1')+
  xlab('Target sample size')+
  g.theme
hplot
```

The plot above is for the `r format(nrow(exclude_large), big.mark=',')` studies with a target sample size under 2,000.


### Histogram looking for digit preference in target sample size

```{r, fig.width=8}
#
exclude_large = filter(studies, samplesize_target <= 200) # exclude extremely large genetics study
# 
digits = group_by(exclude_large, samplesize_target) %>%
  summarise(n=n()) %>%
  ungroup() %>%
  arrange(-n) %>%
  slice(1:10) # top 10
# plot
hplot = ggplot(exclude_large, aes(x=samplesize_target))+
  geom_histogram(fill='chocolate2', breaks=0:200)+
  xlab('Target sample size')+
  geom_label(data=digits, aes(x=samplesize_target, y=n, label=samplesize_target))+
  g.theme
hplot
jpeg('figures/digit_preference.jpg', width=5, height=4, units='in', res=300)
print(hplot)
invisible(dev.off())
```

The plot above is for the `r format(nrow(exclude_large), big.mark=',')` studies with a target sample size of 200 or fewer.

There is a strong digit preference at multiples of ten. The modal sample size is `r digits$samplesize_target[1]`.

### Comparing target and achieved sample size

Below we show a Bland-Altman plot using all the data on the original scale. The results are dominated by a few very large studies, hence we need to re-scale the data.

```{r}
to_plot = filter(studies, !is.na(samplesize_actual)) %>%
  mutate(diff = samplesize_actual - samplesize_target,
         aver = (samplesize_actual + samplesize_target)/2 )
ba.plot = ggplot(data=to_plot, aes(x=aver, y=diff))+
  geom_point(pch=1)+
  xlab('Average sample size')+
  ylab('Difference (Actual minus Target)')+
  g.theme
ba.plot 
```

### Restricted plot to exclude outliers

```{r}
# stats
z = qnorm(0.975)
stats = filter(to_plot, aver < 1000) %>%
  summarise(mean=mean(diff), sd=sd(diff)) %>%
  mutate(lower = mean - z*sd, # limits of agreement
         upper = mean + z*sd)
# plot
ba.plot + coord_cartesian(xlim=c(0,1000), ylim=c(-2000,2000))+
  geom_hline(lty=2, col='red', yintercept = stats$mean)+
  geom_hline(lty=2, col='green', yintercept = stats$lower)+
  geom_hline(lty=2, col='green', yintercept = stats$upper)
```

The Bland-Altman plot above is restricted to exclude the few outliers from very large studies.

### Comparing target and achieved sample size

We use a log-transformation (base e) to remove the strong skew in the sample sizes.

```{r}
# log-transformed version
to_plot = filter(studies, 
                 !is.na(samplesize_actual)) %>% # remove missing
  mutate(
    samplesize_target_log = log(samplesize_target+0.1), # add small constant because of zeros
    samplesize_actual_log = log(samplesize_actual+0.1),
    diff = samplesize_actual_log - samplesize_target_log,
    diff_perc = 100*(exp(diff)-1), # percent difference
    aver = (samplesize_actual_log + samplesize_target_log)/2 )
# check
#filter(to_plot, diff< log(0.1)) %>% select(samplesize_target, samplesize_actual) # where sample was 10% of target
# stats
stats = summarise(to_plot, mean=mean(diff), sd=sd(diff)) %>%
  mutate(lower = mean - z*sd, # limits of agreement
         upper = mean + z*sd)
# label log axis
x.labels = c(1,5,50,500,5000,50000)
x.ticks = log(x.labels+1)
y.labels = c(0.1,0.25,1,4,10) # relative (like a risk ratio); symmetric
y.ticks = log(y.labels)
# plot
ba.plot_log2 = ggplot(data=to_plot, aes(x=aver, y=diff))+
  geom_point(pch=1)+
  scale_x_continuous(breaks=x.ticks, labels=x.labels)+
  scale_y_continuous(breaks=y.ticks, labels=y.labels)+
  xlab('Average sample size (log scale)')+
  ylab('Relative difference (Actual minus Target)')+
  geom_hline(lty=2, col='red', yintercept = stats$mean)+
  geom_hline(lty=2, col='green', yintercept = stats$lower)+
  geom_hline(lty=2, col='green', yintercept = stats$upper)+
  g.theme
ba.plot_log2 
```

The Bland-Altman plot above shows the 95% limits of agreement as horizontal green lines, which are `r roundz(exp(stats$lower),2)` to `r roundz(exp(stats$upper),2)`. 

### Bland-Altman plot for sample size difference using a density plot

The dots in the above plot make it hard to see where most of the observations are.
The plot below shows the density.

```{r}
to_tile = mutate(to_plot,
                 diff_bin = round(diff),
                 diff_aver = round(aver*3)/3) %>%
  group_by(diff_bin, diff_aver) %>%
  summarise(n=n()) %>%
  ungroup()
tile_plot = ggplot(data=to_tile, aes(x=diff_aver, y=diff_bin, fill=n))+
  geom_tile()+
  scale_x_continuous(breaks=x.ticks, labels=x.labels)+
  scale_y_continuous(breaks=y.ticks, labels=y.labels)+
  scale_fill_viridis_c()+
  xlab('Average (log scale)')+
  ylab('Relative difference (Actual minus Target)')+
  g.theme
tile_plot
```

## Regression model of difference against average

We used a Bayesian model to examine the difference in the sample size (actual minus target) against the average sample size.

```{r}
# get the bayesian results
load('results/bland_altman_bayes_model.RData') # from 2_bayes_model_sample_diff.R
res = data.frame(bugs.results$summary[,c(1,3,7)])
names(res) = c('mean','lower','upper'); res$var = row.names(res)
# regression model for tau
reg_parms = filter(res, str_detect(string=var, pattern='alpha|beta'))
reg_parms_nice = mutate(reg_parms, cell = paste(roundz(mean,2), ' (', roundz(lower,2), ' to ', roundz(upper,2), ')', sep=''),
         Term = case_when(
           str_detect(var, 'alpha') ~ 'Mean',
           str_detect(var, 'beta') ~ 'Precision'
         ),
         var = case_when(
           str_detect(var, '1') ~ 'Intercept',
           str_detect(var, '2') ~ 'Slope',
           str_detect(var, '3') ~ 'Quadratic'
         ))  %>%
  select(Term, var, cell) %>%
  rename('Variable' = 'var',
         'Mean (95% CI)' = 'cell')
ftab = flextable(reg_parms_nice) %>%
  theme_box() %>%
  autofit()
ftab
# mean difference and CI (for text below) 
mean.diff = filter(res, var=='alpha')
```

The results above are the estimated parameters from the regression model fitted to the data shown in the Bland-Altman plot.
The regression was the difference against the average, and we allowed both the mean and the precision (inverse-variance) to depend on the average sample size.
The means and 95% CIs from the above table show that both the intercept and quadratic were statistically significant for the mean and precision. So there was a non-linear change in mean and the precision.

The regression model results above are relatively difficult to interpret, so we plot the estimates below. The mean difference is the solid line and the shaded area is a 95% credible interval for the difference which highlights the changing precision. The plot also shows the studies that fall outside the 95% credible interval limits.

```{r}
# un-centre predication locations
preds_av = av.pred + mean_aver
# extract bayes parameters
alpha_1 = filter(reg_parms, var=='alpha[1]')$mean
alpha_2 = filter(reg_parms, var=='alpha[2]')$mean
alpha_3 = filter(reg_parms, var=='alpha[3]')$mean
beta_1 = filter(reg_parms, var=='beta[1]')$mean
beta_2 = filter(reg_parms, var=='beta[2]')$mean
beta_3 = filter(reg_parms, var=='beta[3]')$mean
# add outliers by reconstructing bayes limits
add_outliers = select(to_plot, aver, diff) %>%
  mutate(
  c_aver = aver - mean_aver,
  est_mean = alpha_1 + (c_aver * alpha_2) + (c_aver * c_aver *alpha_3),
  est_tau = beta_1 + (c_aver * beta_2) + (c_aver * c_aver *beta_3),
  est_sigma = 1/sqrt(est_tau),
  z = qnorm(0.90),
  lower = est_mean - (z*est_sigma),
  upper = est_mean + (z*est_sigma)
) %>%
filter(diff > upper | diff < lower)
# to here, add outlier studies; dotted line at 1
to_plot_diff = filter(res, str_detect(string=var, pattern='diff')) %>%
  mutate(xaxis= preds_av)
plot = ggplot(data=to_plot_diff, aes(x=xaxis, y=mean, ymin=lower, ymax=upper))+
  geom_ribbon(alpha=0.2)+
  geom_line(size=1.05)+
  geom_point(data=add_outliers, aes(x=aver, y=diff))+
  scale_x_continuous(breaks=x.ticks, labels=x.labels)+
  scale_y_continuous(breaks=y.ticks, labels=y.labels)+
  xlab('Average (log scale)')+
  ylab('Relative difference (Actual minus Target)')+
  g.theme
plot
```

The averaged difference on the scale of relative was estimated as `r roundz(exp(mean.diff$mean),2)` with a 95% credible interval of `r roundz(exp(mean.diff$lower),2)` to `r roundz(exp(mean.diff$upper),2)`. So on average, sample sizes are `r roundz(100*(exp(mean.diff$mean)-1),2)`% smaller than their target. However, the mean difference is larger for smaller studies, and for larger studies the mean difference is close to 1.

The variance in the difference is narrowest for sample sizes around 500, suggesting studies of this size more often have an actual sample size that is nearer the target.

There are more individual studies that fall outside the credible intervals that are below the shaded area. These are studies that recruited far fewer patients than expected. A lot of these points are between 5 and 500. 

## Multiple regression model of target and actual sample size

```{r, include=FALSE}
## load data/results
# check if results are already available
is_results = length(dir('results', pattern = 'ANZCTR_sample_size')) > 0
if(is_results == TRUE){
  load('results/ANZCTR_sample_size.RData')
}
if(is_results == FALSE){
  source('2_elasticnet_model_samplesize_ANZCTR.R')
}

# get the labels
load('data/labels.RData') # from 0_labels.R
```

We used elastic net. _Add more_.
Selected parsimonious model as minimum CV plus 1 SE.
Checked the final model for colinearity using the variance-inflation factor (VIF).
There were variables that were colinear with study type (Observational/Interventional) and this was part of the reason for running these models separately.

We log-transformed (base e) the sample size because of a very strong positive skew.
The plot shows the estimated mean and 95% confidence interval on the untransformed scale.

### Plot of relative differences for the actual sample size

```{r, fig.width=8, fig.height=9.5}
to_plot = plot_function(indata=all_ests, which_outcome='actual', table_names=table_names_anzctr)
to_plot
# export to tall-thin plot
jpeg('figures/ANZCTR_sample_size_relative_actual.jpg', width=6.5, height=9.5, units='in', res=500)
print(to_plot)
invisible(dev.off())
```

The number of studies was `r format(filter(numbers, outcome=='actual', study_type=='Observational')  %>% pull(n_model), big.mark=',')` for the observational data and `r format(filter(numbers, outcome=='actual', study_type=='Interventional')  %>% pull(n_model), big.mark=',')` for the interventional data.

### Plot of relative differences for the target sample size

```{r, fig.width=8, fig.height=9.5}
to_plot = plot_function(indata=all_ests, which_outcome='target', table_names = table_names_anzctr)
to_plot
# export to tall-thin plot
jpeg('figures/ANZCTR_sample_size_relative_target.jpg', width=6.5, height=9.5, units='in', res=500)
print(to_plot)
invisible(dev.off())
```

The number of studies was `r format(filter(numbers, outcome=='target', study_type=='Observational')  %>% pull(n_model), big.mark=',')` for the observational data and `r format(filter(numbers, outcome=='target', study_type=='Interventional')  %>% pull(n_model), big.mark=',')` for the interventional data.



### Check residuals from regression model

#### a) Target sample size

```{r, fig.width=7, fig.height=3.5}
otype = 'target'
all_res = NULL
for (model in c('Interventional','Observational')){ # get residuals for both types
  model_comb = paste(model, ', ', otype, sep='')
  res = data.frame(res = resid(standard_models[[model_comb]]))
  frame = data.frame(model=model, res=res)
  all_res = bind_rows(all_res, frame)
}
rplot = ggplot(all_res, aes(x=res))+
  geom_histogram(fill='firebrick3')+
  g.theme+
  xlab('Residual (log-scale)')+
  ylab('Count')+
  facet_wrap(~model, scales='free_y')
rplot
# r-squared
# rsq = cor(log2(for.model$samplesize_target), fitted(smodel)) ^2
```

#### b) Actual sample size

```{r, fig.width=7, fig.height=3.5}
otype = 'actual'
all_res = NULL
for (model in c('Interventional','Observational')){ # get residuals for both types
  model_comb = paste(model, ', ', otype, sep='')
  res = data.frame(res = resid(standard_models[[model_comb]]))
  frame = data.frame(model=model, res=res)
  all_res = bind_rows(all_res, frame)
}
rplot = ggplot(all_res, aes(x=res))+
  geom_histogram(fill='firebrick3')+
  g.theme+
  xlab('Residual (log-scale)')+
  ylab('Count')+
  facet_wrap(~model, scales='free_y')
rplot
```


## Common characteristics

Below we show the top ten characteristics of studies in terms of study type, purpose and masking.

```{r, fig.width=8, fig.height=6}
# get combinations 
combs <-  dplyr::select(studies, purpose, masking, study_type, ID)  %>%
  as_tibble() %>%
  tidyr::gather(key='characteristic', value='response', -ID) %>%
  filter(!is.na(response),
         response != 'Not Applicable') %>%
  dplyr::select(-characteristic)
# now make list from group responses
list_resp = group_by(combs, ID) %>%
  summarise(responses = list(response))
# plot
cplot = ggplot(list_resp, aes(x = responses)) +
    geom_bar(aes(y=..count../sum(..count..)), fill = "indianred3") +
    theme_bw() +
    xlab("Study characteristics") +
    ylab("Proportion of studies") +
    scale_x_upset(n_intersections = 10)+
    g.theme+
  theme(text=element_text(size=14),
        plot.margin = margin(t = 0, r = 0, b = 0, l = 120, unit = "pt")) # extend margin for labels
cplot
```


## Principal components analysis

```{r}
library(PCAmixdata)
# to do: would be better to turn age_min and age_max into numbers
# use small subset of variables
for.pca = select(studies, submitted, study_type, assignment, endpoint, control, n_primary, n_secondary ) 
# split by continuous and categorical
split <- splitmix(for.pca) # split into quant and qual data
X1 <- split$X.quanti 
X2 <- split$X.quali 
res.pcamix <- PCAmix(X.quanti=X1, X.quali=X2, rename.level=TRUE, graph=FALSE)
```

```{r, fig.width=8, fig.height=8}
# plot the PCA results
par(mfrow=c(2,2))
plot(res.pcamix,choice="ind",coloring.ind=X2$houses,label=FALSE,
      posleg="bottomright", main="Observations")
plot(res.pcamix, choice="levels", xlim=c(-1.5,2.5), main="Levels")
plot(res.pcamix, choice="cor", main="Numerical variables")
plot(res.pcamix, choice="sqload", coloring.var=T, leg=TRUE,
     posleg="topright", main="All variables")
```

